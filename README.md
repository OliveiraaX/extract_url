# ğŸ“¡ LINK EXTRACTOR & CRAWLER v2.0  
### *Focused Domain Web Crawler â€” ExtraÃ§Ã£o de links por domÃ­nio especÃ­fico*

Este script (`extract.sh`) Ã© um **crawler leve e eficiente**, feito em **Bash**, projetado para **extrair links pertencentes ao mesmo domÃ­nio**, seguindo redirecionamentos, resolvendo IPs e organizando tudo de forma clara no terminal.

Ideal para anÃ¡lises de seguranÃ§a, mapeamento de superfÃ­cie de ataque, identificaÃ§Ã£o de subdomÃ­nios e varreduras de links internas.

---

## âœ¨ Funcionalidades

âœ”ï¸ ExtraÃ§Ã£o de links somente do mesmo domÃ­nio  
âœ”ï¸ DetecÃ§Ã£o e exibiÃ§Ã£o de redirecionamentos  
âœ”ï¸ ResoluÃ§Ã£o de IP com cache (dig, host, getent)  
âœ”ï¸ Fila de crawling com profundidade configurÃ¡vel  
âœ”ï¸ Delay aleatÃ³rio entre requisiÃ§Ãµes para reduzir bloqueios  
âœ”ï¸ Output colorido e organizado  
âœ”ï¸ ContabilizaÃ§Ã£o final de URLs visitadas, links vÃ¡lidos e subdomÃ­nios descobertos  

---

## ğŸ› ï¸ Requisitos

O script necessita das ferramentas:

- `bash`
- `curl`
- `dig` (opcional, mas recomendado)
- `host` (fallback)
- `getent` (fallback)

### InstalaÃ§Ã£o rÃ¡pida em Debian/Ubuntu:

```bash
sudo apt install curl dnsutils bind9-host

## ğŸš€  Uso
Execute o script informando uma URL inicial:
```bash
./extract.sh https://example.com



